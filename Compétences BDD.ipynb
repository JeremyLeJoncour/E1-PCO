{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCO. Création d'une BDD relationnelle démonstrative \n",
    "## Compétences C2. C3. C4. C10.\n",
    "\n",
    "![IllustrationDatabase](Ressources_NB\\IllustrationDatabase.png)\n",
    "\n",
    "Ce fichier présente l'ensemble des scripts de création de BDD, de table, de requêtes d'import/export des données si la base de données était construite directement à partir des fichiers CSV mis à disposition sur le site Kaggle. Le nombre de données étant conséquent, cette BDD \"échantillon\" est réalisée pour la validation des compétences associées. Une autre BDD a été créée spécialement pour la démonstration sur l'application et ayant l'avantage d'effectuer le traitement des requêtes plus rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compétences\n",
    "* **C2.** Concevoir une base de données analytique avec l’approche orientée requêtes en vue de la mise à disposition des données pour un traitement analytique ou d’intelligence artificielle.\n",
    "* **C3.** Programmer l’import de données initiales nécessaires au projet en base de données, afin de les rendre exploitables par un tiers, dans un langage de programmation adapté et à partir de la stratégie de nettoyage des données préalablement définie.\n",
    "* **C4.** Préparer les données disponibles depuis la base de données analytique en vue de leur utilisation par les algorithmes d’intelligence artificielle.\n",
    "* **C10.** Concevoir une base de données relationnelle à l’aide de méthodes standards de modélisation de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies standards de data-analyses:\n",
    "    \n",
    "import numpy as np\n",
    "from numpy import set_printoptions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "# sklearn modules Preprocessing:\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# sklearn modules Model Selection:\n",
    "\n",
    "from sklearn import tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# sklearn modules Model Evaluation & Improvement:\n",
    "    \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "from sklearn.metrics import make_scorer, recall_score, log_loss, matthews_corrcoef\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Réseaux de neurones Tensorflow Keras:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "# Librairies standards de data-visualisation:\n",
    "\n",
    "import seaborn as sn\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "color = sn.color_palette()\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import re\n",
    "\n",
    "\n",
    "# Importation IA:\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Filtre warnings:\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Coloration des graphes Seaborn:\n",
    "\n",
    "colors = ['#440154', '#482475','#414487','#355F8D','#2A788E','#21918C','#22A884','#44BF70','#7AD151','#BDDF26','#FDE725']\n",
    "red = '#fd8181'\n",
    "blue = '#8198fd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction détection des valeurs manquantes:\n",
    "def find_NaN(dataset):\n",
    "    NaN = dataset.isnull().sum().to_frame('Valeurs Manquantes')\n",
    "    NaN = NaN.loc[NaN['Valeurs Manquantes']>0]\n",
    "    NaN['Pourcentage'] = [str(round((i*100)/len(dataset),1)) for i in NaN['Valeurs Manquantes']]\n",
    "    NaN['Pourcentage'] = [i+' %' for i in NaN['Pourcentage']]\n",
    "    if NaN.empty:\n",
    "        return print(f'Aucune valeur manquante n\\'est détectée sur ce dataset')\n",
    "    else:\n",
    "        return NaN\n",
    "    \n",
    "# Fonction transformation date int(YYYYMMDD) en str(YYYY-MM-DD):    \n",
    "def Transform_date(serie):\n",
    "    Liste = []\n",
    "    for i in serie:\n",
    "        i = str(i)\n",
    "        y = i[:4]\n",
    "        m = i[4:6]\n",
    "        d = i[6:]\n",
    "        i = f'{d}-{m}-{y}'\n",
    "        Liste.append(i)\n",
    "\n",
    "    return Liste\n",
    "\n",
    "# Fonction transformation date str(YYYY-MM-DD) en int(YYYYMMDD):    \n",
    "def Format_intdate(serie):\n",
    "    Liste = []\n",
    "    for i in serie:\n",
    "        #i = i.strftime(\"%Y-%m-%d\")\n",
    "        y = i[:4]\n",
    "        m = i[5:7]\n",
    "        d = i[8:]\n",
    "        i = f\"{y}{m}{d}\"\n",
    "        i = int(i)\n",
    "        Liste.append(i)\n",
    "\n",
    "    return Liste\n",
    "\n",
    "def Transform_date_MonthYear(serie):\n",
    "    Liste = []\n",
    "    for i in serie:\n",
    "        i = str(i)\n",
    "        y = i[:4]\n",
    "        m = i[4:6]\n",
    "        i = f'{y}{m}'\n",
    "        Liste.append(i)\n",
    "\n",
    "    return Liste\n",
    "\n",
    "# Différence entre 2 dates en jour:\n",
    "def difference_dates(date1, date2):\n",
    "    return abs(date2-date1).days\n",
    "\n",
    "# Transformation datetime:\n",
    "def TransformationToDate(serie):\n",
    "    new_serie = [datetime.strptime(i, \"%d-%m-%Y\") for i in serie]\n",
    "    return new_serie\n",
    "\n",
    "# Affichage Plot avec 3 axes y sur Logs:\n",
    "def make_patch_spines_invisible(ax):\n",
    "    ax.set_frame_on(True)\n",
    "    ax.patch.set_visible(False)\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "\n",
    "# Affichage Plot Aggregate sur Logs:\n",
    "def plot_aggregate(dataset, churn):\n",
    "    fig, host = plt.subplots(figsize = (15,6))\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "\n",
    "    par1 = host.twinx()\n",
    "    par2 = host.twinx()\n",
    "\n",
    "    par2.spines[\"right\"].set_position((\"axes\", 1.2))\n",
    "\n",
    "    make_patch_spines_invisible(par2)\n",
    "\n",
    "    par2.spines[\"right\"].set_visible(True)\n",
    "\n",
    "    p1, = host.plot(dataset['utilisateur']['2017-03-01':'2017-03-30'].tolist(), label='Nombre Utilisateur', \n",
    "             lw = 3, alpha = 0.5, color = 'blue')\n",
    "    p2, = par1.plot(dataset['sum']['2017-03-01':'2017-03-30'].tolist(), label='Somme d\\'écoute par jour', \n",
    "             lw = 3, ls = '--', alpha = 0.5, color = 'red')\n",
    "    p3, = par2.plot(dataset['mean']['2017-03-01':'2017-03-30'].tolist(), label='Moyenne d\\'écoute par jour', \n",
    "             lw = 2, ls = ':', alpha = 0.5, color = 'purple')\n",
    "\n",
    "    host.set_xlabel(\"Date\")\n",
    "    host.set_ylabel('Nombre Utilisateur', color='blue')\n",
    "    par1.set_ylabel('Somme Temps d\\'écoute en seconde', color='red')\n",
    "    par2.set_ylabel('Moyenne Temps d\\'écoute en seconde', color='purple')\n",
    "\n",
    "    tkw = dict(size=4, width=1.5)\n",
    "    host.tick_params(axis='y', colors=p1.get_color(), **tkw)\n",
    "    par1.tick_params(axis='y', colors=p2.get_color(), **tkw)\n",
    "    par2.tick_params(axis='y', colors=p3.get_color(), **tkw)\n",
    "    host.tick_params(axis='x', **tkw)\n",
    "\n",
    "    lines = [p1, p2, p3]\n",
    "\n",
    "    host.legend(lines, [l.get_label() for l in lines])\n",
    "\n",
    "    plt.title(f'Analyse temporelle sur les utilisateurs {churn}')\n",
    "    plt.show()\n",
    "\n",
    "# Pie Chart Proportion Churn:\n",
    "def proportion_churn(dataset):\n",
    "    labels = ['Abonnés', 'Désabonnés']\n",
    "    colors = [blue, red]\n",
    "    explode = (0, 0.2)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(dataset['is_churn'].value_counts(), startangle=90, colors=colors, wedgeprops={'edgecolor': 'black'}, autopct='%1.f%%', explode = explode, shadow=True)\n",
    "    ax.set_title('Proportion d\\'attrition', fontweight='bold')\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "    fig.legend(loc='lower right', labels=labels, fontsize='medium')\n",
    "    fig.tight_layout()\n",
    "\n",
    "# Proportion Churn:\n",
    "def ratio_churn(dataset, name):\n",
    "    ratio = dataset['is_churn'].value_counts()[1]/len(dataset['is_churn'])\n",
    "    print(f'Proportion désabonnés sur {name} :\\t{round(ratio*100,2)} %')\n",
    "    \n",
    "# Proportion Churn Bar:    \n",
    "def plot_churn(dataset, feature):\n",
    "    ChurnOnFeature = dataset.groupby([feature, 'is_churn']).size().unstack()\n",
    "    ChurnOnFeature.rename(columns={0:'Abonnés', 1:'Désabonnés'}, inplace = True)\n",
    "\n",
    "    colors = ['#8198fd','#fd8181']\n",
    "    ax = (ChurnOnFeature.T*100.0/ChurnOnFeature.T.sum()).T.plot(kind='bar', width = 0.3, stacked = True, rot = 90, figsize = (15, 4), color = colors, grid=False)\n",
    "\n",
    "    plt.ylabel('Proportion de Clients\\n', horizontalalignment=\"center\", fontstyle = \"normal\", fontsize=\"large\", fontfamily = \"sans-serif\")\n",
    "    plt.xlabel(feature+'\\n', horizontalalignment=\"center\", fontstyle = \"normal\", fontsize=\"large\", fontfamily = \"sans-serif\")\n",
    "    plt.title(f'Répartition de l\\'Attrition sur {feature}\\n', horizontalalignment=\"center\", fontstyle = \"normal\", fontsize=\"22\", fontfamily = \"sans-serif\")\n",
    "    plt.legend(loc='upper right', fontsize = \"medium\")\n",
    "    plt.xticks(rotation=0, horizontalalignment=\"center\")\n",
    "    plt.yticks(rotation=0, horizontalalignment=\"right\")\n",
    "\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction des tables Echantillon Base de données principale\n",
    "### Lecture des CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne total :\t\t 18396362\n",
      "Nombre d'utilisateur unique :\t 1103894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>date</th>\n",
       "      <th>num_25</th>\n",
       "      <th>num_50</th>\n",
       "      <th>num_75</th>\n",
       "      <th>num_985</th>\n",
       "      <th>num_100</th>\n",
       "      <th>num_unq</th>\n",
       "      <th>total_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u9E91QDTvHLq6NXjEaWv8u4QIqhrHk72kE+w31Gnhdg=</td>\n",
       "      <td>20170331</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>6309.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nTeWW/eOZA/UHKdD5L7DEqKKFTjaAj3ALLPoAWsU8n0=</td>\n",
       "      <td>20170330</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2390.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2UqkWXwZbIjs03dHLU9KHJNNEvEkZVzm69f3jCS+uLI=</td>\n",
       "      <td>20170331</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>110</td>\n",
       "      <td>23203.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ycwLc+m2O0a85jSLALtr941AaZt9ai8Qwlg9n0Nql5U=</td>\n",
       "      <td>20170331</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>191</td>\n",
       "      <td>7100.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGcbTofOSOkMmQyN1NMLxHEXJ1yV3t/JdhGwQ9wXjnI=</td>\n",
       "      <td>20170331</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>93</td>\n",
       "      <td>28401.558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno      date  num_25  num_50  \\\n",
       "0  u9E91QDTvHLq6NXjEaWv8u4QIqhrHk72kE+w31Gnhdg=  20170331       8       4   \n",
       "1  nTeWW/eOZA/UHKdD5L7DEqKKFTjaAj3ALLPoAWsU8n0=  20170330       2       2   \n",
       "2  2UqkWXwZbIjs03dHLU9KHJNNEvEkZVzm69f3jCS+uLI=  20170331      52       3   \n",
       "3  ycwLc+m2O0a85jSLALtr941AaZt9ai8Qwlg9n0Nql5U=  20170331     176       4   \n",
       "4  EGcbTofOSOkMmQyN1NMLxHEXJ1yV3t/JdhGwQ9wXjnI=  20170331       2       1   \n",
       "\n",
       "   num_75  num_985  num_100  num_unq  total_secs  \n",
       "0       0        1       21       18    6309.273  \n",
       "1       1        0        9       11    2390.699  \n",
       "2       5        3       84      110   23203.337  \n",
       "3       2        2       19      191    7100.454  \n",
       "4       0        1      112       93   28401.558  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logs:\n",
    "df_logs = pd.read_csv('user_logs_v2.csv')\n",
    "logs_copy = df_logs.copy()\n",
    "print(f\"Nombre de ligne total :\\t\\t {len(logs_copy.msno)}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(logs_copy.msno.unique())}\")\n",
    "logs_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne total :\t\t 1431009\n",
      "Nombre d'utilisateur unique :\t 1197050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++6eU4LsQ3UQ20ILS7d99XK8WbiVgbyYL4FUgzZR134=</td>\n",
       "      <td>32</td>\n",
       "      <td>90</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>20170131</td>\n",
       "      <td>20170504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>++lvGPJOinuin/8esghpnqdljm6NXS8m8Zwchc7gOeA=</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>20150809</td>\n",
       "      <td>20190412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+/GXNtXWQVfKrEDqYAzcSw2xSPYMKWNj22m+5XkVQZc=</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>20170303</td>\n",
       "      <td>20170422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+/w1UrZwyka4C9oNH3+Q8fUf3fD8R3EwWrx57ODIsqk=</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>20170329</td>\n",
       "      <td>20170331</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+00PGzKTYqtnb65mPKPyeHXcZEwqiEzktpQksaaSC3c=</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>20170323</td>\n",
       "      <td>20170423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  payment_method_id  \\\n",
       "0  ++6eU4LsQ3UQ20ILS7d99XK8WbiVgbyYL4FUgzZR134=                 32   \n",
       "1  ++lvGPJOinuin/8esghpnqdljm6NXS8m8Zwchc7gOeA=                 41   \n",
       "2  +/GXNtXWQVfKrEDqYAzcSw2xSPYMKWNj22m+5XkVQZc=                 36   \n",
       "3  +/w1UrZwyka4C9oNH3+Q8fUf3fD8R3EwWrx57ODIsqk=                 36   \n",
       "4  +00PGzKTYqtnb65mPKPyeHXcZEwqiEzktpQksaaSC3c=                 41   \n",
       "\n",
       "   payment_plan_days  plan_list_price  actual_amount_paid  is_auto_renew  \\\n",
       "0                 90              298                 298              0   \n",
       "1                 30              149                 149              1   \n",
       "2                 30              180                 180              1   \n",
       "3                 30              180                 180              1   \n",
       "4                 30               99                  99              1   \n",
       "\n",
       "   transaction_date  membership_expire_date  is_cancel  \n",
       "0          20170131                20170504          0  \n",
       "1          20150809                20190412          0  \n",
       "2          20170303                20170422          0  \n",
       "3          20170329                20170331          1  \n",
       "4          20170323                20170423          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transactions:\n",
    "df_transactions = pd.read_csv('transactions_v2.csv')\n",
    "transactions_copy = df_transactions.copy()\n",
    "print(f\"Nombre de ligne total :\\t\\t {len(transactions_copy.msno)}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(transactions_copy.msno.unique())}\")\n",
    "transactions_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne total :\t\t 6769473\n",
      "Nombre d'utilisateur unique :\t 6769473\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rb9UwLQTrxzBVwCB6+bCcSQWZ9JiNLC9dXtM1oEsZA8=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>20110911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+tJonkh+O1CA796Fm5X60UMOtB6POHAwPjbTRVl/EuU=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>20110914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cV358ssn7a0f7jZOwGNWS07wCKVqxyiImJUX6xcIwKw=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>20110915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9bzDeJP6sQodK73K5CBlJ6fgIQzPeLnRl0p5B77XP+g=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>20110915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WFLY3s7z4EZsieHCt63XrsdtfTEmJ+2PnnKLH5GY4Tk=</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>20110915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  city  bd  gender  \\\n",
       "0  Rb9UwLQTrxzBVwCB6+bCcSQWZ9JiNLC9dXtM1oEsZA8=     1   0     NaN   \n",
       "1  +tJonkh+O1CA796Fm5X60UMOtB6POHAwPjbTRVl/EuU=     1   0     NaN   \n",
       "2  cV358ssn7a0f7jZOwGNWS07wCKVqxyiImJUX6xcIwKw=     1   0     NaN   \n",
       "3  9bzDeJP6sQodK73K5CBlJ6fgIQzPeLnRl0p5B77XP+g=     1   0     NaN   \n",
       "4  WFLY3s7z4EZsieHCt63XrsdtfTEmJ+2PnnKLH5GY4Tk=     6  32  female   \n",
       "\n",
       "   registered_via  registration_init_time  \n",
       "0              11                20110911  \n",
       "1               7                20110914  \n",
       "2              11                20110915  \n",
       "3              11                20110915  \n",
       "4               9                20110915  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Members:\n",
    "df_members = pd.read_csv('members_v3.csv')\n",
    "members_copy = df_members.copy()\n",
    "print(f\"Nombre de ligne total :\\t\\t {len(members_copy.msno)}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(members_copy.msno.unique())}\")\n",
    "members_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne total :\t\t 970960\n",
      "Nombre d'utilisateur unique :\t 970960\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8iF/+8HY8lJKFrTc7iR9ZYGCG2Ecrogbc2Vy5YhsfhQ=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K6fja4+jmoZ5xG6BypqX80Uw/XKpMgrEMdG2edFOxnA=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  is_churn\n",
       "0  ugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=         1\n",
       "1  f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=         1\n",
       "2  zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=         1\n",
       "3  8iF/+8HY8lJKFrTc7iR9ZYGCG2Ecrogbc2Vy5YhsfhQ=         1\n",
       "4  K6fja4+jmoZ5xG6BypqX80Uw/XKpMgrEMdG2edFOxnA=         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train:\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "train_copy = df_train.copy()\n",
    "print(f\"Nombre de ligne total :\\t\\t {len(train_copy.msno)}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(train_copy.msno.unique())}\")\n",
    "train_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs :\n",
      "Nombre d'utilisateur unique :\t 1103894\n",
      "Nombre d'utilisateur unique :\t 1103894\n",
      "\n",
      "Transactions :\n",
      "Nombre d'utilisateur unique :\t 1197050\n",
      "Nombre d'utilisateur unique :\t 1197050\n",
      "\n",
      "Members :\n",
      "Nombre d'utilisateur unique :\t 6769473\n",
      "Nombre d'utilisateur unique :\t 6769473\n",
      "\n",
      "Train :\n",
      "Nombre d'utilisateur unique :\t 970960\n",
      "Nombre d'utilisateur unique :\t 970960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Réduction de l'ID utilisateur msno à 12 caractères:\n",
    "\n",
    "logs_copy['msno'] = [i[10:22] for i in logs_copy['msno']]\n",
    "print(\"Logs :\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(df_logs.msno.unique())}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(logs_copy.msno.unique())}\\n\")\n",
    "\n",
    "transactions_copy['msno'] = [i[10:22] for i in transactions_copy['msno']]\n",
    "print(\"Transactions :\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(df_transactions.msno.unique())}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(transactions_copy.msno.unique())}\\n\")\n",
    "\n",
    "members_copy['msno'] = [i[10:22] for i in members_copy['msno']]\n",
    "print(\"Members :\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(df_members.msno.unique())}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(members_copy.msno.unique())}\\n\")\n",
    "\n",
    "train_copy['msno'] = [i[10:22] for i in train_copy['msno']]\n",
    "print(\"Train :\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(df_train.msno.unique())}\")\n",
    "print(f\"Nombre d'utilisateur unique :\\t {len(train_copy.msno.unique())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echantillonnage de démonstration\n",
    "Cette étape présente la marche à suivre pour l'intégration des CSV dans une base de données relationnelle. Le nombre d'instance étant conséquent, **1%** des données sont introduites dans la BDD pour présenter les requêtes de création et d'introduction de la donnée. L'application souhaitant démontrer l'envoie de requêtes instantanés à travers différentes rubriques, le chargement prendrait du temps. Une seconde base de données a été créée avec des données prétraitées (réduisant le temps d'exécution et d'affichage des résultats dans l'application).\n",
    "\n",
    "La méthode *Train_Test_Split* est réalisée pour récupérer **1%** des données sur le fichier Train, contenant des ID client unique (*msno*). La séparation est faites de telle sorte que le ratio de la variable cible *is_churn* soit concervé. Les variables temporelles (format *str*) observées en amont sont converties en *datetime* pour leur future insertion en BDD.\n",
    "\n",
    "![schema_1](Ressources_NB\\schema_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste d'utilisateurs uniques : 9709\n",
      "Taille pour Logs : 137644\n",
      "Taille pour Transactions : 11388\n",
      "Taille pour Members : 8629\n",
      "Taille pour Train : 9709\n"
     ]
    }
   ],
   "source": [
    "# Split:\n",
    "r =  40\n",
    "Train_sample, REM = train_test_split(train_copy, train_size=0.01, stratify = train_copy['is_churn'], random_state = r)\n",
    "sample_user = Train_sample['msno'].unique()\n",
    "print(f'Liste d\\'utilisateurs uniques : {len(sample_user)}')\n",
    "\n",
    "# Echantillonnage logs:\n",
    "logs = logs_copy[logs_copy['msno'].isin(sample_user)]\n",
    "print(f'Taille pour Logs : {len(logs)}')\n",
    "\n",
    "# Echantillonnage transactions:\n",
    "transactions = transactions_copy[transactions_copy['msno'].isin(sample_user)]\n",
    "print(f'Taille pour Transactions : {len(transactions)}')\n",
    "\n",
    "# Echantillonnage members:\n",
    "members = members_copy[members_copy['msno'].isin(sample_user)]\n",
    "print(f'Taille pour Members : {len(members)}')\n",
    "\n",
    "# Echantillonnage train:\n",
    "train = train_copy[train_copy['msno'].isin(sample_user)]\n",
    "print(f'Taille pour Train : {len(train)}')\n",
    "\n",
    "# Conversion des variables dates en vu de leur intégration en base SQL:\n",
    "intermate_date1 = []\n",
    "for i in logs['date']:\n",
    "    s = str(i)\n",
    "    date = datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8])).strftime('%Y:%m:%d')\n",
    "    intermate_date1.append(date)\n",
    "\n",
    "logs['date'] = intermate_date1\n",
    "\n",
    "intermate_date2 = []\n",
    "for i in transactions['transaction_date']:\n",
    "    s = str(i)\n",
    "    date = datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8])).strftime('%Y:%m:%d')\n",
    "    intermate_date2.append(date)\n",
    "\n",
    "transactions['transaction_date'] = intermate_date2\n",
    "\n",
    "intermate_date3 = []\n",
    "for i in transactions['membership_expire_date']:\n",
    "    s = str(i)\n",
    "    date = datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8])).strftime('%Y:%m:%d')\n",
    "    intermate_date3.append(date)\n",
    "\n",
    "transactions['membership_expire_date'] = intermate_date3\n",
    "\n",
    "intermate_date4 = []\n",
    "for i in members['registration_init_time']:\n",
    "    s = str(i)\n",
    "    date = datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8])).strftime('%Y:%m:%d')\n",
    "    intermate_date4.append(date)\n",
    "\n",
    "members['registration_init_time'] = intermate_date4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling ID User\n",
    "Pour faire la connexion entre les différentes tables, un ID de type *integer* est attribué pour chaque identifiant *msno* unique. La nouvelle variable est donc mergée avec les DataFrames *logs*, *transaction* et *members*. \n",
    "\n",
    "![schema_2](Ressources_NB\\schema_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateurs uniques sur df_user : 9709\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['msno'].tolist())\n",
    "train['ID_user'] = le.transform(train['msno'].tolist())\n",
    "train['ID_user'] = [i+1 for i in train['ID_user']]\n",
    "\n",
    "print(f'Utilisateurs uniques sur df_user : {len(train.ID_user.unique())}')\n",
    "\n",
    "df_user = train[['ID_user', 'msno', 'is_churn']]\n",
    "df_user = df_user.sort_values(by=['ID_user'], ascending = True)\n",
    "df_user = df_user.reset_index(drop=True)\n",
    "\n",
    "# Merging Transactions ID_USER:\n",
    "transactions = pd.merge(df_user, transactions, on='msno', how='inner')\n",
    "transactions = transactions.drop(['is_churn'], 1)\n",
    "\n",
    "# Merging Logs ID_USER:\n",
    "logs = pd.merge(df_user, logs, on='msno', how='inner')\n",
    "logs = logs.drop(['is_churn'], 1)\n",
    "\n",
    "# Merging Members ID_USER, members contiendra aussi la variable cible is_churn:\n",
    "members = pd.merge(df_user, members, on='msno', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création BDD C2.\n",
    "Cette BDD relationnelle représente les 4 fichiers CSV d'origine. L'ensemble des données peuvent être inséré directement par les requêtes suivantes (ici comme mentionné, 0.5% des utilisateurs sont introduits, mais le schéma reste le même).\n",
    "\n",
    "![schema_3](Ressources_NB\\schema_3.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import mysql.connector\n",
    "\n",
    "config = {\n",
    "      'user': 'root',\n",
    "      'password': 'root',\n",
    "      'host': 'localhost',\n",
    "      'port': '3307',\n",
    "    }\n",
    "\n",
    "# Création du Cursor\n",
    "link = mysql.connector.connect(**config)\n",
    "cursor = link.cursor(buffered=True)\n",
    "\n",
    "cursor.execute(\"\"\"CREATE DATABASE DBPCO\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des Tables C10.\n",
    "Ces tables reprennent la structure des 4 fichiers CSV. Une table User comprend les Identifiants *msno* et chiffré nommé *User_ID* qui fait le lien avec les autres tables. Les 3 autres tables comprennent respectivement les données de *logs*, *transactions* et *members*. Cette dernière table contient aussi la *target is_churn*."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = {\n",
    "          'user': 'root',\n",
    "          'password': 'root',\n",
    "          'host': 'localhost',\n",
    "          'port': '3307',\n",
    "          'database': 'DBPCO', \n",
    "        }\n",
    "\n",
    "# Création du Cursor\n",
    "link = mysql.connector.connect(**config)\n",
    "cursor = link.cursor(buffered=True)\n",
    "\n",
    "# Création des tables de la BDD relationnelle:\n",
    "cursor.execute(\"\"\"CREATE TABLE User (\n",
    "                    User_ID INT,\n",
    "                    msno VARCHAR(55),\n",
    "                    PRIMARY KEY (User_ID)\n",
    "                )\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE Transactions (\n",
    "                    Transactions_ID INT AUTO_INCREMENT,\n",
    "                    payment_method_id INT,\n",
    "                    payment_plan_days INT, \n",
    "                    plan_list_price INT, \n",
    "                    actual_amount_paid INT, \n",
    "                    is_auto_renew INT, \n",
    "                    transaction_date DATE, \n",
    "                    membership_expire_date DATE,\n",
    "                    is_cancel INT, \n",
    "                    User_ID INT,\n",
    "                    PRIMARY KEY (Transactions_ID),\n",
    "                    FOREIGN KEY fk_user(User_ID)\n",
    "                    REFERENCES User(User_ID)\n",
    "                    ON UPDATE CASCADE\n",
    "                    ON DELETE RESTRICT\n",
    "                )\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE Members (\n",
    "                    Members_ID INT AUTO_INCREMENT,\n",
    "                    city INT, \n",
    "                    bd INT, \n",
    "                    gender VARCHAR(55), \n",
    "                    registered_via INT, \n",
    "                    registration_init_time DATE,\n",
    "                    is_churn INT,\n",
    "                    User_ID INT,\n",
    "                    PRIMARY KEY (Members_ID),\n",
    "                    FOREIGN KEY fk_user(User_ID)\n",
    "                    REFERENCES User(User_ID)\n",
    "                    ON UPDATE CASCADE\n",
    "                    ON DELETE RESTRICT\n",
    "                )\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE Logs (\n",
    "                    Logs_ID INT AUTO_INCREMENT,\n",
    "                    date  DATE,\n",
    "                    num_25 INT, \n",
    "                    num_50 INT, \n",
    "                    num_75 INT, \n",
    "                    num_985 INT, \n",
    "                    num_100 INT, \n",
    "                    num_unq INT,\n",
    "                    total_secs INT, \n",
    "                    User_ID int,\n",
    "                    PRIMARY KEY (Logs_ID),\n",
    "                    FOREIGN KEY fk_user(User_ID)\n",
    "                    REFERENCES User(User_ID)\n",
    "                    ON UPDATE CASCADE\n",
    "                    ON DELETE RESTRICT\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation C3.\n",
    "L'ensemble des requêtes pour inserer les données dans la BDD relationnelle type MySQL."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Table Users:\n",
    "for i in range(len(df_user)):\n",
    "    user_id = df_user['ID_user'][i]\n",
    "    msno = df_user['msno'][i]\n",
    "    \n",
    "    table = \"INSERT INTO User (User_ID, msno) VALUES (%s, %s)\"\n",
    "    values = (int(user_id),  str(msno))\n",
    "    \n",
    "    cursor.execute(table, values)\n",
    "    link.commit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Table Transactions:\n",
    "for i in range(len(transactions)):\n",
    "    payment_method_id = transactions['payment_method_id'][i]\n",
    "    payment_plan_days = transactions['payment_plan_days'][i]\n",
    "    plan_list_price = transactions['plan_list_price'][i]\n",
    "    actual_amount_paid = transactions['actual_amount_paid'][i]\n",
    "    is_auto_renew = transactions['is_auto_renew'][i]\n",
    "    transaction_date = transactions['transaction_date'][i]\n",
    "    membership_expire_date = transactions['membership_expire_date'][i]\n",
    "    is_cancel = transactions['is_cancel'][i]\n",
    "    user_id = transactions['ID_user'][i]\n",
    "\n",
    "    \n",
    "    table = \"\"\"INSERT INTO Transactions (Transactions_ID, payment_method_id, payment_plan_days, plan_list_price, \n",
    "    actual_amount_paid, is_auto_renew, transaction_date, membership_expire_date, is_cancel, \n",
    "    User_ID) VALUES (NULL, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    \n",
    "    values = (int(payment_method_id), int(payment_plan_days), int(plan_list_price), int(actual_amount_paid), \n",
    "              int(is_auto_renew), str(transaction_date), str(membership_expire_date), int(is_cancel), int(user_id))\n",
    "    \n",
    "    cursor.execute(table, values)\n",
    "    link.commit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Table Members:\n",
    "for i in range(len(members)):\n",
    "    city = members['city'][i]\n",
    "    bd = members['bd'][i]\n",
    "    gender = members['gender'][i]\n",
    "    registered_via = members['registered_via'][i]\n",
    "    registration_init_time = members['registration_init_time'][i]\n",
    "    is_churn = members['is_churn'][i]\n",
    "    user_id = members['ID_user'][i]\n",
    "\n",
    "    \n",
    "    table = \"\"\"INSERT INTO Members (Members_ID, city, bd, gender, registered_via, registration_init_time, \n",
    "    is_churn, User_ID) VALUES (NULL, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    \n",
    "    values = (int(city), int(bd), str(gender), int(registered_via), \n",
    "              str(registration_init_time),int(is_churn), int(user_id))\n",
    "    \n",
    "    cursor.execute(table, values)\n",
    "    link.commit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Table Logs:\n",
    "for i in range(len(logs)):\n",
    "    date = logs['date'][i]\n",
    "    num_25 = logs['num_25'][i]\n",
    "    num_50 = logs['num_50'][i]\n",
    "    num_75 = logs['num_75'][i]\n",
    "    num_985 = logs['num_985'][i]\n",
    "    num_100 = logs['num_100'][i]\n",
    "    num_unq = logs['num_unq'][i]\n",
    "    total_secs = logs['total_secs'][i]\n",
    "    user_id = logs['ID_user'][i]\n",
    "\n",
    "    \n",
    "    table = \"\"\"INSERT INTO Logs (Logs_ID, date, num_25, num_50, num_75, num_985, \n",
    "    num_100, num_unq, total_secs, User_ID) VALUES (NULL, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    \n",
    "    values = (str(date), int(num_25), int(num_50), int(num_75), \n",
    "              int(num_985), int(num_100), int(num_unq), float(total_secs), int(user_id))\n",
    "    \n",
    "    cursor.execute(table, values)\n",
    "    link.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export des tables C4.\n",
    "L'ensemble des requêtes qui font appellent aux données présentes dans la BDD relationnelle afin de les retransformer en DataFrame de même configuration que les fichiers CSV de départ. Une jointure est effectuée avec la table User pour afficher l'identifiant *msno* plutôt que l'identifiant chiffré *User_ID*. Ces requêtes permettent l'analyse théorique des données (si toutefois, l'ensemble des données étaient dans la BDD).\n",
    "\n",
    "![schema_4](Ressources_NB\\schema_4.png)\n",
    "\n",
    "**L'export des tables en Dataframes permet l'analyse de ces dernières**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à la BDD:\n",
    "config = {\n",
    "          'user': 'root',\n",
    "          'password': 'root',\n",
    "          'host': 'localhost',\n",
    "          'port': '3307',\n",
    "          'database': 'DBPCO', \n",
    "        }\n",
    "\n",
    "# Création du Cursor\n",
    "link = mysql.connector.connect(**config)\n",
    "cursor = link.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requêtes formation dataframe Transactions:\n",
    "query = \"\"\" SELECT \n",
    "                    Transactions.payment_method_id, \n",
    "                    Transactions.payment_plan_days, \n",
    "                    Transactions.plan_list_price,\n",
    "                    Transactions.actual_amount_paid, \n",
    "                    Transactions.is_auto_renew, \n",
    "                    Transactions.transaction_date, \n",
    "                    Transactions.membership_expire_date,\n",
    "                    Transactions.is_cancel, \n",
    "                    User.msno\n",
    "            \n",
    "            FROM Transactions JOIN User on Transactions.User_ID = User.User_ID\n",
    "        \"\"\"\n",
    "cursor.execute(query)\n",
    "rows_transactions = cursor.fetchall()\n",
    "\n",
    "Transactions_sql = []\n",
    "for values in rows_transactions :\n",
    "    Transactions_sql.append(list(values))\n",
    "\n",
    "transaction_columns = ['payment_method_id', 'payment_plan_days', 'plan_list_price','actual_amount_paid', \n",
    "                       'is_auto_renew', 'transaction_date', 'membership_expire_date','is_cancel', 'msno']\n",
    "transaction_export = pd.DataFrame(Transactions_sql, columns = transaction_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requêtes formation dataframe Logs:\n",
    "query = \"\"\" SELECT \n",
    "                    Logs.date, \n",
    "                    Logs.num_25, \n",
    "                    Logs.num_50, \n",
    "                    Logs.num_75, \n",
    "                    Logs.num_985, \n",
    "                    Logs.num_100, \n",
    "                    Logs.num_unq, \n",
    "                    Logs.total_secs, \n",
    "                    User.msno\n",
    "            \n",
    "            FROM Logs JOIN User on Logs.User_ID = User.User_ID\n",
    "        \"\"\"\n",
    "cursor.execute(query)\n",
    "rows_logs = cursor.fetchall()\n",
    "\n",
    "Logs_sql = []\n",
    "for values in rows_logs :\n",
    "    Logs_sql.append(list(values))\n",
    "\n",
    "log_columns = ['date', 'num_25', 'num_50', 'num_75', 'num_985', 'num_100', 'num_unq', 'total_secs', 'msno']\n",
    "log_export = pd.DataFrame(Logs_sql, columns = log_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requêtes formation dataframe members:\n",
    "query = \"\"\" SELECT \n",
    "                    Members.city, \n",
    "                    Members.bd, \n",
    "                    Members.gender, \n",
    "                    Members.registered_via, \n",
    "                    Members.registration_init_time,\n",
    "                    Members.is_churn,\n",
    "                    User.msno\n",
    "            \n",
    "            FROM Members JOIN User on Members.User_ID = User.User_ID\n",
    "        \"\"\"\n",
    "cursor.execute(query)\n",
    "rows_members = cursor.fetchall()\n",
    "link.close()\n",
    "\n",
    "Members_sql = []\n",
    "for values in rows_members :\n",
    "    Members_sql.append(list(values))\n",
    "\n",
    "member_columns = ['city', 'bd', 'gender', 'registered_via', 'registration_init_time', 'is_churn', 'msno']\n",
    "member_export = pd.DataFrame(Members_sql, columns = member_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Le modèle créé en amont fonctionnait sur des données ayant subit certains traitements (réduction des valeurs aberrantes, suppression de données manquantes, regroupement etc...). Les scripts présentent un condensé du travail de ***preprocessing*** effectué durant l'analyse générale.\n",
    "\n",
    "### Logs\n",
    "Représentant le nombre de musique et le temps d'écoute journalier pour chaque utilisateur, les informations ont été reportées sur le mois. Les nouvelles variables correspondent à la somme et moyenne d'écoute/musiques écoutées. Une variable *count* repertorie le nombre de jour de connexion de chaque utilisateur.\n",
    "\n",
    "Les valeurs aberrantes ont été détectées en appliquant un seuil (Z-score). Ces données ont été changées en la médiane de chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Logs:\n",
    "\n",
    "Features = [log_export.num_25, log_export.num_50, log_export.num_75, log_export.num_985, log_export.num_100, \n",
    "            log_export.num_unq, log_export.total_secs]\n",
    "name_features = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100', 'num_unq', 'total_secs']\n",
    "\n",
    "Outliers_del = []\n",
    "\n",
    "for i, name in zip(Features, name_features) :\n",
    "    Identify_outliers = []\n",
    "    \n",
    "    # Calcul du Z Score\n",
    "    log_export['Z_score']=(i-i.mean())/i.std()\n",
    "    z = len(log_export[(log_export['Z_score']>3)])\n",
    "    \n",
    "    # Identification des valeurs aberrantes\n",
    "    Outliers_del.append(log_export[(log_export['Z_score']>3) | (log_export['Z_score']<-3)].shape[0])\n",
    "    Identify_outliers = log_export[(log_export['Z_score']>3) | (log_export['Z_score']<-3)].index\n",
    "    R_with_Median = log_export[(log_export['Z_score']<3) & (log_export['Z_score']>-3)][name].median()\n",
    "    \n",
    "    # Remplacement par la médiane\n",
    "    log_export.loc[Identify_outliers, name] = R_with_Median\n",
    "    \n",
    "log_export = log_export.drop(['Z_score'], 1)\n",
    "\n",
    "# Regroupement des informations sur le mois, une instance correspond à un utilisateur unique:\n",
    "LogsSum = log_export.groupby('msno').sum().reset_index()\n",
    "LogsSum.rename(columns={'num_25':'num_25Sum','num_50':'num_50Sum','num_75':'num_75Sum','num_985':'num_985Sum',\n",
    "                        'num_100':'num_100Sum','num_unq':'num_unqSum','total_secs':'total_secsSum'}, inplace = True)\n",
    "\n",
    "LogsMean = log_export.groupby('msno').mean().reset_index()\n",
    "LogsMean.rename(columns={'num_25':'num_25Mean','num_50':'num_50Mean','num_75':'num_75Mean','num_985':'num_985Mean',\n",
    "                         'num_100':'num_100Mean','num_unq':'num_unqMean','total_secs':'total_secsMean'}, inplace = True)\n",
    "\n",
    "days_count = log_export.groupby('msno').size().to_frame('count')\n",
    "days_count = pd.merge(log_export, days_count, on='msno', how='inner')\n",
    "days_count = days_count[['msno', 'count']]\n",
    "days_count = days_count.groupby('msno').mean()\n",
    "\n",
    "Preprocess_logs = pd.merge(LogsSum, LogsMean, on='msno', how='inner')\n",
    "Preprocess_logs = pd.merge(Preprocess_logs, days_count, on='msno', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions\n",
    "La variable représentant le nombre de transaction sur le mois a été créée et ajouté au DataFrame, au vu d'un futur report sur le mois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de transactions par utilisateur:\n",
    "transaction_count = transaction_export.groupby('msno').size().to_frame('transaction_count')\n",
    "transaction_count = pd.merge(transaction_export, transaction_count, on='msno', how='inner')\n",
    "transaction_count = transaction_count[['msno', 'transaction_count']]\n",
    "transaction_count = transaction_count.groupby('msno').mean()\n",
    "\n",
    "Preprocess_transaction = pd.merge(transaction_export, transaction_count, on='msno', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Members\n",
    "Les valeurs manquantes de départ sur la variable *gender*, concervé dans la base de données, ont été remplacées par le terme \"inconnu\".\n",
    "\n",
    "Pour l'âge, les valeurs aberrantes situées en dehors d'un intervalle raisonnable de 10 à 70 ans ont été fixé à -1. Le choix d'un remplacement plutôt qu'une suppression s'explique par une variabilité certaine entre les utilisateurs restants abonnés et les utilisateurs désabonnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquantes de la variable gender par 'inconnu'\n",
    "member_export['gender'] = member_export['gender'].replace(to_replace =\"nan\", value =\"inconnu\")\n",
    "\n",
    "# Nettoyage des valeurs abérrantes:\n",
    "member_export['bd'] = [i if 10 < i < 70 else -1 for i in member_export['bd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "Les tables \"converties\" en Dataframes sont fusionnées. De nouvelles variables sont créées à l'occasion en effectuant des rapports entre plusieurs variables etc... Les données de variables temporelles sont converties en *integer*. Finalement, chaque instance correspond à un utilisateur unique, après l'application d'un filtre retenant, la dernière date de transaction effectuée.\n",
    "\n",
    "Dans le cadre de ce projet, c'est ce genre de Dataframe qui a été introduit dans une autre BDD relationnelle connectée à l'application de démonstration.\n",
    "\n",
    "![schema_5](Ressources_NB\\schema_5.png)\n",
    "\n",
    "### Base de données relationnelle avec preprocessing sur 43544 Utilisateurs uniques (Dataset Test)\n",
    "**BDD utilisée pour la démonstration de l'application**\n",
    "![schema_6](Ressources_NB\\schema_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'utilisateurs unique : 7323 pour 7323 lignes\n"
     ]
    }
   ],
   "source": [
    "# Merging et Groupby msno sur membership_expire_date:\n",
    "Dataset = pd.merge(Preprocess_transaction, member_export, on='msno', how='inner')\n",
    "Dataset = pd.merge(Dataset, Preprocess_logs, on='msno', how='inner')\n",
    "\n",
    "\n",
    "# Feature Engineering___ Prix abonnement / Temps abonnement:\n",
    "Price_per_day = []\n",
    "for i, j in zip(Dataset.plan_list_price, Dataset.payment_plan_days):\n",
    "    try:\n",
    "        k = i/j\n",
    "        Price_per_day.append(k)\n",
    "    except:\n",
    "        Price_per_day.append(0)\n",
    "\n",
    "Dataset['price_per_day'] = Price_per_day\n",
    "\n",
    "# Feature Engineering___ Différences entre membership_expire_date et registration_init_time en jour:\n",
    "Dataset['days_fidelity'] = [difference_dates(i, j) for i, j in zip(Dataset.membership_expire_date, Dataset.registration_init_time)]\n",
    "\n",
    "# Reconversion des Dates en INT:\n",
    "Dataset['membership_expire_date'] = [int(i.strftime(\"%Y%m%d\")) for i in Dataset['membership_expire_date']]\n",
    "Dataset['registration_init_time'] = [int(i.strftime(\"%Y%m%d\")) for i in Dataset['registration_init_time']]\n",
    "Dataset['transaction_date'] = [int(i.strftime(\"%Y%m%d\")) for i in Dataset['transaction_date']]\n",
    "\n",
    "# Réduction sur la dernière date de transaction:\n",
    "Dataset = Dataset.loc[Dataset.groupby('msno')['membership_expire_date'].idxmax()]\n",
    "\n",
    "print(f'Nombre d\\'utilisateurs unique : {len(Dataset.msno.unique())} pour {len(Dataset)} lignes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Model\n",
    "La démarche est similaire que lors de la création des modèles IA. Le Dataframe final est traité une deuxième fois pour encoder les variables catégorielles telles que *gender*, *payment_method_id*, *registered_via*, *city*. Par la suite, l'entrainement du modèle peut commencé si, bien entendu, la BDD créé ici contenait l'ensemble des données.\n",
    "\n",
    "L'analyse et la modélisation ayant été effectuées en amont directement à partir des fichiers CSV, plusieurs fichiers Joblib ont été enregistré. Ils comprennent :\n",
    "* Le modèle XGBoost Classifier Optimisé après Feature Sélection.\n",
    "* L'échelle de Standardisation *fit* sur les données Train et permettant de transformer de nouvelles données.\n",
    "* ColonneScaler, une liste des variables présentent lors de la Standardisation des données d'entraînement.\n",
    "* ColonnesModel, une liste des variables sélectionnés (*feature_selection*) pour les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des Fichiers de Modèles et Scaler:\n",
    "scaler = joblib.load('ProcessModel/ScalerXGBC_BF.joblib')\n",
    "colonnesmodel = joblib.load('ProcessModel/ColonnesXGBC_BF.joblib')\n",
    "classifier = joblib.load('ProcessModel/XGBC_BF.joblib')\n",
    "colonnescaler = joblib.load('ProcessModel/ColonnesForScale.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune valeur manquante n'est détectée sur ce dataset\n"
     ]
    }
   ],
   "source": [
    "# Modification de series catégorielles:\n",
    "df = Dataset.copy()\n",
    "\n",
    "df['city'] = [str(i)+'C' for i in df.city]\n",
    "df['payment_method_id'] = [str(i)+'P' for i in df.payment_method_id]\n",
    "df['registered_via'] = [str(i)+'R' for i in df.registered_via]\n",
    "\n",
    "# Encodage:\n",
    "df = pd.concat([df, pd.get_dummies(df.gender)],1)\n",
    "df = pd.concat([df, pd.get_dummies(df.payment_method_id)],1)\n",
    "df = pd.concat([df, pd.get_dummies(df.registered_via)],1)\n",
    "df = pd.concat([df, pd.get_dummies(df.city)],1)\n",
    "df = df.drop(['gender', 'payment_method_id','registered_via', 'city'], 1)\n",
    "\n",
    "\n",
    "# Calibration avec le dataset de départ ayant servi à la modélisation: \n",
    "for feature in colonnescaler:\n",
    "        if feature not in df.columns:\n",
    "            df[feature]=0\n",
    "            \n",
    "df = df[colonnescaler]\n",
    "\n",
    "# Vérification:\n",
    "find_NaN(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du modèle (Démonstration NB)\n",
    "Démontre que la suite des processus, débutant sur l'ensemble des requêtes pour la création des nouveaux Dataframes depuis la BDD jusqu'aux travaux de prétraitements forme un jeu de donnée cohérent et pouvant être passé dans le modèle IA créé en amont sans erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "X_test = df.drop(['msno','is_churn'], axis = 1)\n",
    "y_test = df['is_churn']\n",
    "\n",
    "# Standardisation des données Test avec le scaler fit sur données d'entrainement pour l'intégration Web\n",
    "X_test2 = pd.DataFrame(scaler.transform(X_test.values))\n",
    "X_test2.columns = X_test.columns.values\n",
    "X_test2.index = X_test.index.values\n",
    "X_test = X_test2\n",
    "\n",
    "X_test = X_test[colonnesmodel]\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "probability = classifier.predict_proba(X_test)\n",
    "probability = probability[:,1]\n",
    "\n",
    "final_results = df\n",
    "final_results['predictions'] = y_pred\n",
    "final_results['membership_expire_date'] = df['membership_expire_date']\n",
    "final_results[\"propensity_to_churn(%)\"] = probability\n",
    "final_results[\"propensity_to_churn(%)\"] = final_results[\"propensity_to_churn(%)\"]*100\n",
    "final_results[\"propensity_to_churn(%)\"] = final_results[\"propensity_to_churn(%)\"].astype(int)\n",
    "final_results = final_results.sort_values(by=['propensity_to_churn(%)'], ascending = False)\n",
    "Results = final_results[['msno', 'is_churn', 'propensity_to_churn(%)']]\n",
    "Results.index.msno=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats\n",
    "Application sommaire du modèle IA sur le Dataset construit à partir de cette BDD relationnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC :\t\t0.99\n",
      "Logloss :\t0.06\n",
      "F1 :\t\t0.83\n",
      "Precision :\t0.95\n",
      "Recall :\t0.74\n",
      "MCC :\t\t0.83\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, probability)\n",
    "logloss = log_loss(y_test, probability)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"\"\"AUC :\\t\\t{round(auc,2)}\\nLogloss :\\t{round(logloss,2)}\\nF1 :\\t\\t{round(f1,2)}\\nPrecision :\\t{round(precision,2)}\\nRecall :\\t{round(recall,2)}\\nMCC :\\t\\t{round(mcc,2)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row0_col2 {\n",
       "            background-color:  #67000d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row1_col2 {\n",
       "            background-color:  #67000d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row2_col2 {\n",
       "            background-color:  #6b010e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row3_col2 {\n",
       "            background-color:  #9a0c14;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row4_col2 {\n",
       "            background-color:  #b71319;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row5_col2 {\n",
       "            background-color:  #b71319;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row6_col2 {\n",
       "            background-color:  #c3161b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row7_col2 {\n",
       "            background-color:  #c9181d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row8_col2 {\n",
       "            background-color:  #cc191e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row9_col2 {\n",
       "            background-color:  #ed392b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row10_col2 {\n",
       "            background-color:  #ed392b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row11_col2 {\n",
       "            background-color:  #ed392b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row12_col2 {\n",
       "            background-color:  #f24734;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row13_col2 {\n",
       "            background-color:  #f96245;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row14_col2 {\n",
       "            background-color:  #f96245;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row15_col2 {\n",
       "            background-color:  #fc8464;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row16_col2 {\n",
       "            background-color:  #fc8464;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row17_col2 {\n",
       "            background-color:  #fc8767;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row18_col2 {\n",
       "            background-color:  #fc8e6e;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row19_col2 {\n",
       "            background-color:  #fc9272;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row20_col2 {\n",
       "            background-color:  #fc9272;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row21_col2 {\n",
       "            background-color:  #fcb398;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row22_col2 {\n",
       "            background-color:  #fcb79c;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row23_col2 {\n",
       "            background-color:  #fdc6b0;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row24_col2 {\n",
       "            background-color:  #fee4d8;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row25_col2 {\n",
       "            background-color:  #fee4d8;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row26_col2 {\n",
       "            background-color:  #fee4d8;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row27_col2 {\n",
       "            background-color:  #fff2eb;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row28_col2 {\n",
       "            background-color:  #fff5f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f9c376d2_8feb_11ec_a004_68f7286f4513row29_col2 {\n",
       "            background-color:  #fff5f0;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >msno</th>        <th class=\"col_heading level0 col1\" >is_churn</th>        <th class=\"col_heading level0 col2\" >propensity_to_churn(%)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row0\" class=\"row_heading level0 row0\" >5094</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row0_col0\" class=\"data row0 col0\" >Xdk6lr1w+CTJ</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row0_col2\" class=\"data row0 col2\" >99</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row1\" class=\"row_heading level0 row1\" >1979</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row1_col0\" class=\"data row1 col0\" >BCm4b9AW1ZP3</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row1_col2\" class=\"data row1 col2\" >99</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row2\" class=\"row_heading level0 row2\" >5005</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row2_col0\" class=\"data row2 col0\" >X9gUkD598ov0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row2_col2\" class=\"data row2 col2\" >98</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row3\" class=\"row_heading level0 row3\" >8322</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row3_col0\" class=\"data row3 col0\" >uQ5rnorFRfTY</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row3_col2\" class=\"data row3 col2\" >89</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row4\" class=\"row_heading level0 row4\" >7260</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row4_col0\" class=\"data row4 col0\" >mZFZzOn+yH4F</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row4_col2\" class=\"data row4 col2\" >81</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row5\" class=\"row_heading level0 row5\" >2589</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row5_col0\" class=\"data row5 col0\" >FeuDXL3Kcb50</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row5_col2\" class=\"data row5 col2\" >81</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row6\" class=\"row_heading level0 row6\" >6336</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row6_col0\" class=\"data row6 col0\" >gJhiiGWz7GQU</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row6_col2\" class=\"data row6 col2\" >77</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row7\" class=\"row_heading level0 row7\" >382</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row7_col0\" class=\"data row7 col0\" >0s4t24vcHNYC</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row7_col2\" class=\"data row7 col2\" >75</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row8\" class=\"row_heading level0 row8\" >5803</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row8_col0\" class=\"data row8 col0\" >crz21yx2XVnS</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row8_col2\" class=\"data row8 col2\" >74</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row9\" class=\"row_heading level0 row9\" >72</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row9_col0\" class=\"data row9 col0\" >+YfwO9pqidtb</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row9_col2\" class=\"data row9 col2\" >63</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row10\" class=\"row_heading level0 row10\" >7172</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row10_col0\" class=\"data row10 col0\" >lrDJfhzAevea</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row10_col1\" class=\"data row10 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row10_col2\" class=\"data row10 col2\" >63</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row11\" class=\"row_heading level0 row11\" >8088</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row11_col0\" class=\"data row11 col0\" >snGhHvWt5JcD</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row11_col1\" class=\"data row11 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row11_col2\" class=\"data row11 col2\" >63</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row12\" class=\"row_heading level0 row12\" >4163</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row12_col0\" class=\"data row12 col0\" >Qppt04Y4RrLr</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row12_col1\" class=\"data row12 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row12_col2\" class=\"data row12 col2\" >59</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row13\" class=\"row_heading level0 row13\" >7264</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row13_col0\" class=\"data row13 col0\" >mZXl0tNPLL0C</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row13_col2\" class=\"data row13 col2\" >52</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row14\" class=\"row_heading level0 row14\" >7697</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row14_col0\" class=\"data row14 col0\" >pi8gavxiwnEX</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row14_col1\" class=\"data row14 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row14_col2\" class=\"data row14 col2\" >52</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row15\" class=\"row_heading level0 row15\" >3437</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row15_col0\" class=\"data row15 col0\" >LhEf8h0Aul4K</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row15_col1\" class=\"data row15 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row15_col2\" class=\"data row15 col2\" >42</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row16\" class=\"row_heading level0 row16\" >7621</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row16_col0\" class=\"data row16 col0\" >pBmIaSn9znlA</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row16_col1\" class=\"data row16 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row16_col2\" class=\"data row16 col2\" >42</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row17\" class=\"row_heading level0 row17\" >4983</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row17_col0\" class=\"data row17 col0\" >X+FdohIPFxfz</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row17_col1\" class=\"data row17 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row17_col2\" class=\"data row17 col2\" >41</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row18\" class=\"row_heading level0 row18\" >2489</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row18_col0\" class=\"data row18 col0\" >EdaCWnDko1wE</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row18_col1\" class=\"data row18 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row18_col2\" class=\"data row18 col2\" >39</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row19\" class=\"row_heading level0 row19\" >4324</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row19_col0\" class=\"data row19 col0\" >SDsG5L7w7qpF</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row19_col1\" class=\"data row19 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row19_col2\" class=\"data row19 col2\" >38</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row20\" class=\"row_heading level0 row20\" >7230</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row20_col0\" class=\"data row20 col0\" >mJLotK8v81xZ</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row20_col1\" class=\"data row20 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row20_col2\" class=\"data row20 col2\" >38</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row21\" class=\"row_heading level0 row21\" >4753</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row21_col0\" class=\"data row21 col0\" >Va6+zaeREhxL</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row21_col1\" class=\"data row21 col1\" >1</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row21_col2\" class=\"data row21 col2\" >28</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row22\" class=\"row_heading level0 row22\" >2259</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row22_col0\" class=\"data row22 col0\" >DC6grPwHeiT5</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row22_col1\" class=\"data row22 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row22_col2\" class=\"data row22 col2\" >27</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row23\" class=\"row_heading level0 row23\" >1136</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row23_col0\" class=\"data row23 col0\" >5esIfRGvkKOc</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row23_col1\" class=\"data row23 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row23_col2\" class=\"data row23 col2\" >22</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row24\" class=\"row_heading level0 row24\" >2845</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row24_col0\" class=\"data row24 col0\" >HPAYFqQg8Qae</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row24_col1\" class=\"data row24 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row24_col2\" class=\"data row24 col2\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row25\" class=\"row_heading level0 row25\" >5361</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row25_col0\" class=\"data row25 col0\" >Zhk1aKBLYTJv</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row25_col1\" class=\"data row25 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row25_col2\" class=\"data row25 col2\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row26\" class=\"row_heading level0 row26\" >7203</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row26_col0\" class=\"data row26 col0\" >m1i6QdAFMWU7</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row26_col1\" class=\"data row26 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row26_col2\" class=\"data row26 col2\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row27\" class=\"row_heading level0 row27\" >1872</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row27_col0\" class=\"data row27 col0\" >Ac9OFNuLG4W8</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row27_col1\" class=\"data row27 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row27_col2\" class=\"data row27 col2\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row28\" class=\"row_heading level0 row28\" >230</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row28_col0\" class=\"data row28 col0\" >/ioVwkYpdytD</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row28_col1\" class=\"data row28 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row28_col2\" class=\"data row28 col2\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513level0_row29\" class=\"row_heading level0 row29\" >7064</th>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row29_col0\" class=\"data row29 col0\" >l/kKUdVBytd0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row29_col1\" class=\"data row29 col1\" >0</td>\n",
       "                        <td id=\"T_f9c376d2_8feb_11ec_a004_68f7286f4513row29_col2\" class=\"data row29 col2\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1efa07b69a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Echantillon des prédictions obtenues:\n",
    "Table = pd.DataFrame(columns = Results.columns)\n",
    "for i in range (10, 110, 10):\n",
    "    j = i - 10\n",
    "    Filter = Results.loc[Results['propensity_to_churn(%)'] < i]\n",
    "    Sample = Filter.loc[Filter['propensity_to_churn(%)'] > j].sample(n = 3)\n",
    "    Table = Table.append(Sample)\n",
    "    \n",
    "Table = Table.sort_values(by = ['propensity_to_churn(%)'], ascending = [False])\n",
    "Table.style.background_gradient(cmap=\"Reds\", subset=['propensity_to_churn(%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
